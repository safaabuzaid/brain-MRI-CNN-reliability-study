# Evaluating the Clinical Reliability of Deep Learning Models for Brain Tumor Classification Using MRI

This repository explores **whether deep learning models that achieve high accuracy on brain tumor MRI classification actually make clinically meaningful decisions.**
The project combines CNN models with **explainable AI technique** to understand *where the model looks and whether that focus aligns with real medical features.*

<img width="1020" height="249" alt="Image" src="https://github.com/user-attachments/assets/1e3ba5e9-e9e1-4037-a6f6-d52a44cc080c" />

---

## Introduction

Deep learning models achieve impressive accuracy in medical imaging tasks, but most still operate as “black boxes,” making clinicians hesitant to trust them. In domains like brain MRI diagnosis—where decisions must be transparent—explainability is essential.

This project explores how reliably a CNN model classifies brain tumors by combining performance evaluation with interpretability tools such as Grad-CAM. The goal is simple:

to check whether the model is truly learning tumor-related features or relying on irrelevant patterns.

By visualizing the regions that influence the model’s predictions, the project compares:

- What the model focuses on, vs.
- What actually matters clinically.
  
This helps reveal whether high accuracy reflects real understanding or just statistical shortcuts.

### Research Question

**Does high classification accuracy in CNN models for brain tumor MRI represent real clinical understanding, or does the model depend on non-diagnostic image regions?**

---

## Goals of the Project

1. Build a baseline **CNN model** (ResNet18 + EfficientNet) for multi-class brain tumor MRI classification.
2. Improve performance and robustness through preprocessing, handling class imbalance, and model architecture improvements.
3. Use **Grad-CAM** to visualize the model’s attention during predictions.
4. Compare correct vs. incorrect predictions to understand model failure modes.
5. Identify cases where the model focuses on irrelevant regions (skull, edges, other artifacts).
6. Highlight why deep learning accuracy can be misleading in medical imaging tasks.

---

## Methodology

<img width="1345" height="417" alt="Image" src="https://github.com/user-attachments/assets/057383e1-9c3d-4377-94a7-1c7ad2e2bcd9" />

1. **Data Exploration**
    - Inspect dataset structure, class balance, MRI quality, and artifacts.
2. **Preprocessing**
    - Normalization, resizing, augmentation, class balancing strategies.
3. **Model Training**
    - Baseline model: ResNet-18
    - Modified model: EfficientNet
    - Compare accuracy and general performance.
4. **Explainability Analysis**
    - Apply Grad-CAM on test images.
    - Inspect model attention and save annotated heatmaps.
5. **Error Analysis**
    - Create a CSV (gradcam_analysis.csv) summarizing predictions, errors, focus regions, and clinical insights.
    - Manual review to understand whether the model looked at tumor regions or misleading areas.
6. **Interpretation & Insights**
    - Analyze cases where the model performs well but for the wrong reasons.
    - Highlight why such models are unreliable for clinical deployment.

---

## Repository Structure

```markdown
├── README.md                    # Project documentation               
│
├── notebooks/                   #Google Colab notebooks
│   ├── 01_DataExploration.ipynb          
│   ├── 02_DataPreprocessing.ipynb
│   ├── 03_Augmentaion_CNN_model.ipynb
│   ├── 04_Model_Iprovement.ipynb
│   └── 05_ExplainableAI.ipynb          
│
├── models/                      #Saved models weight
│   ├── brain_tumor_resnet18.pth  
│   └── efficient_full_model.pth                 
│
├── results/                     #Output generated by the model               
    ├── gradcam/                                    
    └── gradcam_analysis.csv             

```

---

## Example Output

**Example explanation of model prediction using Grad-CAM:**

<img width="500" height="300" alt="Image" src="https://github.com/user-attachments/assets/8da1b47e-419f-46f6-9014-c5536316aced" />

- Model correctly detected **Glioma**; but Grad-CAM focuses on **skull edges.**
  

<img width="500" height="300" alt="Image" src="https://github.com/user-attachments/assets/53762e76-4582-441a-907a-81cb59609d76" />

- Model **misclassified** pituitary as meningioma; focus almost to tumor and it's surrounding tissues.
  

<img width="500" height="300" alt="Image" src="https://github.com/user-attachments/assets/194746b7-82e8-45d8-82bd-d54d3f1130f2" />  

- Model misclassified Glioma as meningioma; focus drifted to the background.

---

## Future Work

- Test additional explainability tools (Integrated Gradients, SHAP).
- Investigate training changes (segmentation-guided models, tumor masks).
- Explore clinically grounded evaluation metrics.
- Build a small report or paper summarizing findings.  

---

## Author

**Safaa Osman**  
MSc in Biomedical Engineering— Medical Image Processing  
Interested in AI for medical imaging and interpretable deep learning.

---

## License

You are welcome to use and adapt the code with proper attribution.
